{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e857c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc284df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3b0a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 - load CSV and province list\n",
    "\n",
    "CSV_FN = \"disaster_articless.csv\"   # change if different\n",
    "PH_JS_PATH = Path(\"static/phprovinces.js\")  # attempt to read province names from this file\n",
    "\n",
    "# --------------- load CSV ---------------\n",
    "if not Path(CSV_FN).exists():\n",
    "    raise FileNotFoundError(f\"CSV file not found at {CSV_FN}. Put your CSV in same directory as notebook.\")\n",
    "df = pd.read_csv(CSV_FN, dtype=str).fillna(\"\")\n",
    "\n",
    "# Expect columns: Date, Headline, Keyword, Link, Tags, Abstract, Article\n",
    "print(\"Loaded\", len(df), \"rows from\", CSV_FN)\n",
    "\n",
    "# --------------- load province names ---------------\n",
    "def load_provinces_from_ph_js(js_path):\n",
    "    \"\"\"\n",
    "    If you have a JS file that defines a var like `var phProvinces = {...};`,\n",
    "    we attempt to extract the JSON and collect feature.properties.NAME_1 values.\n",
    "    \"\"\"\n",
    "    s = Path(js_path).read_text(encoding=\"utf-8\")\n",
    "    # naive extraction: find first '{' and last '}' and try to json.loads\n",
    "    start = s.find('{')\n",
    "    end = s.rfind('}')\n",
    "    if start == -1 or end == -1:\n",
    "        return []\n",
    "    json_text = s[start:end+1]\n",
    "    # JS may use single quotes or trailing commas; perform minor cleanup:\n",
    "    json_text = re.sub(r\"(\\w+):\", r'\"\\1\":', json_text)  # crude: bareword keys => quoted (risky)\n",
    "    # remove `var phProvinces =` if exists - we've sliced\n",
    "    try:\n",
    "        geo = json.loads(json_text)\n",
    "        names = []\n",
    "        for f in geo.get(\"features\", []):\n",
    "            props = f.get(\"properties\", {})\n",
    "            # several possible keys used earlier: NAME_1, adm2_en etc.\n",
    "            for key in (\"NAME_1\", \"name\", \"adm2_en\", \"Prov\"): \n",
    "                if key in props and props[key]:\n",
    "                    names.append(props[key])\n",
    "                    break\n",
    "        return list(dict.fromkeys([n.strip() for n in names if n]))\n",
    "    except Exception as e:\n",
    "        # fallback no provinces loaded\n",
    "        print(\"Could not parse phprovinces.js as JSON:\", e)\n",
    "        return []\n",
    "\n",
    "provinces = []\n",
    "if PH_JS_PATH.exists():\n",
    "    try:\n",
    "        provinces = load_provinces_from_ph_js(PH_JS_PATH)\n",
    "    except Exception as e:\n",
    "        print(\"Error loading provinces from JS:\", e)\n",
    "\n",
    "# Fallback list (shortened common provinces). If you want exhaustive list, replace below with a full list.\n",
    "if not provinces:\n",
    "    provinces = [\n",
    "        \"Abra\",\"Agusan del Norte\",\"Agusan del Sur\",\"Aklan\",\"Albay\",\"Antique\",\"Apayao\",\"Aurora\",\n",
    "        \"Basilan\",\"Bataan\",\"Batanes\",\"Batangas\",\"Benguet\",\"Biliran\",\"Bohol\",\"Bukidnon\",\"Bulacan\",\n",
    "        \"Cagayan\",\"Camarines Norte\",\"Camarines Sur\",\"Camiguin\",\"Capiz\",\"Catanduanes\",\"Cavite\",\n",
    "        \"Cebu\",\"Cotabato\",\"Davao del Norte\",\"Davao del Sur\",\"Davao de Oro\",\"Davao Occidental\",\"Davao Oriental\",\n",
    "        \"Dinagat Islands\",\"Eastern Samar\",\"Guimaras\",\"Ifugao\",\"Ilocos Norte\",\"Ilocos Sur\",\"Iloilo\",\"Isabela\",\n",
    "        \"Kalinga\",\"La Union\",\"Laguna\",\"Lanao del Norte\",\"Lanao del Sur\",\"Leyte\",\"Maguindanao\",\"Marinduque\",\n",
    "        \"Masbate\",\"Metro Manila\",\"Misamis Occidental\",\"Misamis Oriental\",\"Mountain Province\",\"Negros Occidental\",\n",
    "        \"Negros Oriental\",\"Northern Samar\",\"Nueva Ecija\",\"Nueva Vizcaya\",\"Occidental Mindoro\",\"Oriental Mindoro\",\n",
    "        \"Palawan\",\"Pampanga\",\"Pangasinan\",\"Quezon\",\"Quirino\",\"Rizal\",\"Romblon\",\"Samar\",\"Sarangani\",\"Siquijor\",\n",
    "        \"Sorsogon\",\"South Cotabato\",\"Southern Leyte\",\"Sultan Kudarat\",\"Surigao del Norte\",\"Surigao del Sur\",\n",
    "        \"Tarlac\",\"Tawi-Tawi\",\"Zambales\",\"Zamboanga del Norte\",\"Zamboanga del Sur\",\"Zamboanga Sibugay\"\n",
    "    ]\n",
    "\n",
    "# Normalize provinces (for matching)\n",
    "provinces_normalized = [p.lower() for p in provinces]\n",
    "print(f\"Using {len(provinces_normalized)} provinces for detection (sample):\", provinces[:8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31933c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3 - load model and tokenizer (uses GPU if available)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "MODEL_NAME = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Summarization helper with chunking\n",
    "def summarize_text(text, max_input_tokens=1024, max_summary_tokens=150, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Token-based chunking: split long input into chunks of max_input_tokens - overlap,\n",
    "    summarize each chunk, then summarize concatenated chunk-summaries into one final summary.\n",
    "    \"\"\"\n",
    "    if not text or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    # Tokenize full text (list of token ids)\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    n = len(tokens)\n",
    "    if n <= max_input_tokens:\n",
    "        inputs = tokenizer.encode_plus(text, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(input_ids, num_beams=4, max_length=max_summary_tokens, early_stopping=True)\n",
    "        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    # chunk\n",
    "    summaries = []\n",
    "    stride = max_input_tokens - chunk_overlap\n",
    "    for i in range(0, n, stride):\n",
    "        chunk_ids = tokens[i : i + max_input_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "        inputs = tokenizer.encode_plus(chunk_text, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(input_ids, num_beams=4, max_length=max_summary_tokens, early_stopping=True)\n",
    "        chunk_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(chunk_summary)\n",
    "        if i + max_input_tokens >= n:\n",
    "            break\n",
    "    # combine chunk summaries into final summary\n",
    "    combined = \" \".join(summaries)\n",
    "    # final compress summarization (optional)\n",
    "    inputs = tokenizer.encode_plus(combined, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        final_ids = model.generate(input_ids, num_beams=4, max_length=max_summary_tokens, early_stopping=True)\n",
    "    final_summary = tokenizer.decode(final_ids[0], skip_special_tokens=True)\n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac069c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4 - disaster classification by keywords\n",
    "disaster_type_keywords = {\n",
    "    \"flooding\": [\"flood\", \"flooding\", \"floods\", \"inundation\", \"water level\", \"submerged\", \"flash flood\", \"flooded\"],\n",
    "    \"typhoon\": [\"typhoon\", \"super typhoon\", \"bagyo\", \"storm\", \"tropical storm\", \"landfall\"],\n",
    "    \"earthquake\": [\"earthquake\", \"tremor\", \"aftershock\", \"magnitude\", \"richter\"],\n",
    "    \"fire\": [\"fire\", \"blaze\", \"wildfire\", \"burned\", \"arson\", \"inferno\", \"smoke\"],\n",
    "    \"volcanic eruption\": [\"volcano\", \"eruption\", \"ashfall\", \"lahar\", \"pyroclastic\", \"volcanic\"],\n",
    "}\n",
    "\n",
    "# Normalize keywords to lowercase\n",
    "for k in disaster_type_keywords:\n",
    "    disaster_type_keywords[k] = [w.lower() for w in disaster_type_keywords[k]]\n",
    "\n",
    "def classify_disaster_types(text):\n",
    "    \"\"\"\n",
    "    Returns a list of disaster type labels detected in text using keyword matching.\n",
    "    If none matched, returns [\"other\"].\n",
    "    \"\"\"\n",
    "    text_l = (text or \"\").lower()\n",
    "    found = set()\n",
    "    for dtype, kws in disaster_type_keywords.items():\n",
    "        for kw in kws:\n",
    "            if kw in text_l:\n",
    "                found.add(dtype)\n",
    "                break\n",
    "    if not found:\n",
    "        # fallback: look for words like 'evacuation' or 'calamity' etc. to mark as 'other'\n",
    "        if any(x in text_l for x in [\"evacuate\", \"evacuee\", \"calamity\", \"disaster\", \"incident\"]):\n",
    "            return [\"other\"]\n",
    "        return []\n",
    "    return sorted(found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 - province detection helpers\n",
    "\n",
    "def detect_provinces(text, provinces_list=provinces):\n",
    "    \"\"\"\n",
    "    Returns a list of province names (original-cased) that appear in text.\n",
    "    Uses approximate substring matching: tokenized words compared to province tokens.\n",
    "    \"\"\"\n",
    "    t = (text or \"\").lower()\n",
    "    matches = set()\n",
    "    # exact substring match for province names\n",
    "    for p in provinces:\n",
    "        if p.lower() in t:\n",
    "            matches.add(p)\n",
    "    # additional heuristic: check common short forms (Metro Manila / Manila)\n",
    "    if \"manila\" in t and \"Metro Manila\" in provinces:\n",
    "        matches.add(\"Metro Manila\")\n",
    "    return sorted(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6 - main processing\n",
    "OUTPUT_JSON = \"static/data/summaries_by_province.json\"\n",
    "OUTPUT_CSV = \"static/data/summaries_flat.csv\"\n",
    "\n",
    "results = []   # flat list of processed records\n",
    "\n",
    "# iterate rows (use tqdm)\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    article_text = row.get(\"Article\") or row.get(\"Abstract\") or row.get(\"Headline\") or \"\"\n",
    "    if not article_text or article_text.strip()==\"\":\n",
    "        continue\n",
    "    # optionally use Headline + Abstract + Article to give more context\n",
    "    combined_text = \" \".join([str(row.get(c,\"\")) for c in [\"Headline\",\"Abstract\",\"Article\"] if row.get(c)])\n",
    "    # classify disaster types (keyword rules)\n",
    "    types = classify_disaster_types(combined_text)\n",
    "    # detect provinces mentioned in text\n",
    "    provinces_found = detect_provinces(combined_text)\n",
    "    # if no provinces found, try to look in 'Tags' or 'Link' or fallback to 'unknown'\n",
    "    if not provinces_found:\n",
    "        tags = (row.get(\"Tags\") or \"\").lower()\n",
    "        for p in provinces:\n",
    "            if p.lower() in tags:\n",
    "                provinces_found.append(p)\n",
    "    if not provinces_found:\n",
    "        provinces_found = [\"Unknown\"]\n",
    "\n",
    "    # summarize (skip if extremely short)\n",
    "    summary = summarize_text(combined_text)\n",
    "    \n",
    "    record = {\n",
    "        \"index\": int(idx),\n",
    "        \"Date\": row.get(\"Date\",\"\"),\n",
    "        \"Headline\": row.get(\"Headline\",\"\"),\n",
    "        \"Link\": row.get(\"Link\",\"\"),\n",
    "        \"Detected_Provinces\": provinces_found,\n",
    "        \"Detected_Types\": types if types else [\"other\"],\n",
    "        \"Summary\": summary\n",
    "    }\n",
    "    results.append(record)\n",
    "\n",
    "# save flat CSV\n",
    "pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False, encoding=\"UTF-8\")\n",
    "print(\"Wrote flat results to\", OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7 - aggregate\n",
    "from collections import defaultdict\n",
    "\n",
    "agg = defaultdict(lambda: defaultdict(list))\n",
    "for r in results:\n",
    "    for p in r[\"Detected_Provinces\"]:\n",
    "        for t in r[\"Detected_Types\"]:\n",
    "            agg[p][t].append({\n",
    "                \"Date\": r[\"Date\"],\n",
    "                \"Headline\": r[\"Headline\"],\n",
    "                \"Link\": r[\"Link\"],\n",
    "                \"Summary\": r[\"Summary\"]\n",
    "            })\n",
    "\n",
    "# Convert to plain dict for JSON serialization\n",
    "agg_out = {p: {t: vals for t, vals in types.items()} for p, types in agg.items()}\n",
    "\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(agg_out, fh, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Wrote aggregated summaries to\", OUTPUT_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b884ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 - quick check and example display\n",
    "import json\n",
    "with open(OUTPUT_JSON, encoding=\"utf-8\") as fh:\n",
    "    data = json.load(fh)\n",
    "\n",
    "# Show provinces that have entries\n",
    "for prov, types in list(data.items())[:10]:\n",
    "    print(prov, \"->\", {t: len(v) for t, v in types.items()})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
