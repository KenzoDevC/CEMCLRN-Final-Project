{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e857c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc284df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "CSV_FN = \"disaster_articless.csv\" \n",
    "df = pd.read_csv(CSV_FN, dtype=str).fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cede0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 2 - Label province\n",
    "\n",
    "# Provinces\n",
    "provinces = [\n",
    "    \"Abra\",\"Agusan del Norte\",\"Agusan del Sur\",\"Aklan\",\"Albay\",\"Antique\",\"Apayao\",\"Aurora\",\n",
    "    \"Basilan\",\"Bataan\",\"Batanes\",\"Batangas\",\"Benguet\",\"Biliran\",\"Bohol\",\"Bukidnon\",\"Bulacan\",\n",
    "    \"Cagayan\",\"Camarines Norte\",\"Camarines Sur\",\"Camiguin\",\"Capiz\",\"Catanduanes\",\"Cavite\",\n",
    "    \"Cebu\",\"Cotabato\",\"Davao del Norte\",\"Davao del Sur\",\"Davao de Oro\",\"Davao Occidental\",\"Davao Oriental\",\n",
    "    \"Dinagat Islands\",\"Eastern Samar\",\"Guimaras\",\"Ifugao\",\"Ilocos Norte\",\"Ilocos Sur\",\"Iloilo\",\"Isabela\",\n",
    "    \"Kalinga\",\"La Union\",\"Laguna\",\"Lanao del Norte\",\"Lanao del Sur\",\"Leyte\",\"Maguindanao del Norte\", \n",
    "    \"Maguindanao del Sur\",\"Marinduque\",\"Masbate\",\"Metro Manila\",\"Misamis Occidental\",\"Misamis Oriental\",\n",
    "    \"Mountain Province\",\"Negros Occidental\",\"Negros Oriental\",\"Northern Samar\",\"Nueva Ecija\",\"Nueva Vizcaya\",\n",
    "    \"Occidental Mindoro\",\"Oriental Mindoro\",\"Palawan\",\"Pampanga\",\"Pangasinan\",\"Quezon\",\"Quirino\",\"Rizal\",\n",
    "    \"Romblon\",\"Samar\",\"Sarangani\",\"Siquijor\",\"Sorsogon\",\"South Cotabato\",\"Southern Leyte\",\"Sultan Kudarat\",\n",
    "    \"Surigao del Norte\",\"Surigao del Sur\",\"Tarlac\",\"Tawi-Tawi\",\"Zambales\",\"Zamboanga del Norte\",\n",
    "    \"Zamboanga del Sur\",\"Zamboanga Sibugay\"\n",
    "]\n",
    "\n",
    "# Metro Manila city → province map\n",
    "metro_manila_cities = [\n",
    "    \"Pasay\", \"Makati\", \"Quezon City\", \"Mandaluyong\", \"Caloocan\",\n",
    "    \"Manila\", \"Navotas\", \"Taguig\", \"Parañaque\", \"Marikina\", \"Pasig\",\n",
    "    \"Valenzuela\", \"Las Piñas\", \"Muntinlupa\", \"Pateros\", \"Malabon\"\n",
    "]\n",
    "city_to_province = {\n",
    "    \"Quezon City\": \"Metro Manila\",\n",
    "    \"Makati\": \"Metro Manila\",\n",
    "    \"Manila\": \"Metro Manila\",\n",
    "    \"Pasay\": \"Metro Manila\",\n",
    "    \"Caloocan\": \"Metro Manila\",\n",
    "    \"Davao City\": \"Davao del Sur\",\n",
    "    \"Cagayan de Oro\": \"Misamis Oriental\",\n",
    "    # brute force manually add ICC, CC, and HUC\n",
    "}\n",
    "\n",
    "import re\n",
    "\n",
    "def detect_provinces_hybrid(text):\n",
    "    \"\"\"\n",
    "    Detect provinces where the disaster actually occurred,\n",
    "    ignoring news source locations (e.g., Manila).\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    found_provinces = set()\n",
    "\n",
    "    # Rule-based: explicit province names\n",
    "    for prov in provinces:\n",
    "        if prov.lower() in text_lower:\n",
    "            found_provinces.add(prov)\n",
    "    \n",
    "    # Rule-based: city-to-province mapping, ignore source cities\n",
    "    for city, prov in city_to_province.items():\n",
    "        if \"MANILA\":\n",
    "            continue  # skip news source mentions\n",
    "        if city.lower() in text_lower:\n",
    "            found_provinces.add(prov)\n",
    "    \n",
    "    # ML fallback if still empty\n",
    "    if not found_provinces:\n",
    "        found_provinces.add(\"Unknown\")\n",
    "    \n",
    "    return list(found_provinces)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31933c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 3 - load model and tokenizer (uses GPU if available)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "MODEL_NAME = \"facebook/bart-large-cnn\"\n",
    "tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = BartForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Summarization helper with chunking\n",
    "def summarize_text(text, max_input_tokens=1024, max_summary_tokens=150, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Token-based chunking: split long input into chunks of max_input_tokens - overlap,\n",
    "    summarize each chunk, then summarize concatenated chunk-summaries into one final summary.\n",
    "    \"\"\"\n",
    "    if not text or text.strip() == \"\":\n",
    "        return \"\"\n",
    "    # Tokenize full text (list of token ids)\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    n = len(tokens)\n",
    "    if n <= max_input_tokens:\n",
    "        inputs = tokenizer.encode_plus(text, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(input_ids, num_beams=4, max_length=max_summary_tokens, early_stopping=True)\n",
    "        return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    # chunk\n",
    "    summaries = []\n",
    "    stride = max_input_tokens - chunk_overlap\n",
    "    for i in range(0, n, stride):\n",
    "        chunk_ids = tokens[i : i + max_input_tokens]\n",
    "        chunk_text = tokenizer.decode(chunk_ids, skip_special_tokens=True)\n",
    "        inputs = tokenizer.encode_plus(chunk_text, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens)\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            summary_ids = model.generate(input_ids, num_beams=4, max_length=max_summary_tokens, early_stopping=True)\n",
    "        chunk_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        summaries.append(chunk_summary)\n",
    "        if i + max_input_tokens >= n:\n",
    "            break\n",
    "    # combine chunk summaries into final summary\n",
    "    combined = \" \".join(summaries)\n",
    "    # final compress summarization (optional)\n",
    "    inputs = tokenizer.encode_plus(combined, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens)\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        final_ids = model.generate(input_ids, num_beams=4, max_length=max_summary_tokens, early_stopping=True)\n",
    "    final_summary = tokenizer.decode(final_ids[0], skip_special_tokens=True)\n",
    "    return final_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac069c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 4 - disaster classification by keywords\n",
    "disaster_type_keywords = {\n",
    "    \"flooding\": [\"flood\", \"flooding\", \"floods\", \"inundation\", \"water level\", \"submerged\", \"flash flood\", \"flooded\"],\n",
    "    \"typhoon\": [\"typhoon\", \"super typhoon\", \"bagyo\", \"storm\", \"tropical storm\", \"landfall\"],\n",
    "    \"earthquake\": [\"earthquake\", \"tremor\", \"aftershock\", \"magnitude\", \"richter\"],\n",
    "    \"fire\": [\"fire\", \"blaze\", \"wildfire\", \"burned\", \"arson\", \"inferno\", \"smoke\"],\n",
    "    \"volcanic eruption\": [\"volcano\", \"eruption\", \"ashfall\", \"lahar\", \"pyroclastic\", \"volcanic\"],\n",
    "}\n",
    "\n",
    "# Normalize keywords to lowercase\n",
    "for k in disaster_type_keywords:\n",
    "    disaster_type_keywords[k] = [w.lower() for w in disaster_type_keywords[k]]\n",
    "\n",
    "def classify_disaster_types(text):\n",
    "    \"\"\"\n",
    "    Returns a list of disaster type labels detected in text using keyword matching.\n",
    "    If none matched, returns [\"other\"].\n",
    "    \"\"\"\n",
    "    text_l = (text or \"\").lower()\n",
    "    found = set()\n",
    "    for dtype, kws in disaster_type_keywords.items():\n",
    "        for kw in kws:\n",
    "            if kw in text_l:\n",
    "                found.add(dtype)\n",
    "                break\n",
    "    if not found:\n",
    "        # fallback: look for words like 'evacuation' or 'calamity' etc. to mark as 'other'\n",
    "        if any(x in text_l for x in [\"evacuate\", \"evacuee\", \"calamity\", \"disaster\", \"incident\"]):\n",
    "            return [\"other\"]\n",
    "        return []\n",
    "    return sorted(found)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a89fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 5 - province detection helpers\n",
    "\n",
    "def detect_provinces(text, provinces_list=provinces):\n",
    "    \"\"\"\n",
    "    Returns a list of province names (original-cased) that appear in text.\n",
    "    Uses approximate substring matching: tokenized words compared to province tokens.\n",
    "    \"\"\"\n",
    "    t = (text or \"\").lower()\n",
    "    matches = set()\n",
    "    # exact substring match for province names\n",
    "    for p in provinces:\n",
    "        if p.lower() in t:\n",
    "            matches.add(p)\n",
    "    # additional heuristic: check common short forms (Metro Manila / Manila)\n",
    "    if \"manila\" in t and \"Metro Manila\" in provinces:\n",
    "        matches.add(\"Metro Manila\")\n",
    "    return sorted(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bb33de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 6 - main processing\n",
    "OUTPUT_JSON = \"static/data/summaries_by_province.json\"\n",
    "OUTPUT_CSV = \"static/data/summaries_flat.csv\"\n",
    "\n",
    "results = []   # flat list of processed records\n",
    "\n",
    "# iterate rows (use tqdm)\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    article_text = row.get(\"Article\") or row.get(\"Abstract\") or row.get(\"Headline\") or \"\"\n",
    "    if not article_text or article_text.strip()==\"\":\n",
    "        continue\n",
    "    # optionally use Headline + Abstract + Article to give more context\n",
    "    combined_text = \" \".join([str(row.get(c,\"\")) for c in [\"Headline\",\"Abstract\",\"Article\"] if row.get(c)])\n",
    "    # classify disaster types (keyword rules)\n",
    "    types = classify_disaster_types(combined_text)\n",
    "    provinces_found = detect_provinces_hybrid(combined_text)\n",
    "\n",
    "    # summarize (skip if extremely short)\n",
    "    summary = summarize_text(combined_text)\n",
    "    \n",
    "    record = {\n",
    "        \"index\": int(idx),\n",
    "        \"Date\": row.get(\"Date\",\"\"),\n",
    "        \"Headline\": row.get(\"Headline\",\"\"),\n",
    "        \"Link\": row.get(\"Link\",\"\"),\n",
    "        \"Detected_Provinces\": provinces_found,\n",
    "        \"Detected_Types\": types if types else [\"other\"],\n",
    "        \"Summary\": summary\n",
    "    }\n",
    "    results.append(record)\n",
    "\n",
    "# save flat CSV\n",
    "pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False, encoding=\"UTF-8\")\n",
    "print(\"Wrote flat results to\", OUTPUT_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7 - aggregate\n",
    "from collections import defaultdict\n",
    "\n",
    "agg = defaultdict(lambda: defaultdict(list))\n",
    "for r in results:\n",
    "    for p in r[\"Detected_Provinces\"]:\n",
    "        for t in r[\"Detected_Types\"]:\n",
    "            agg[p][t].append({\n",
    "                \"Date\": r[\"Date\"],\n",
    "                \"Headline\": r[\"Headline\"],\n",
    "                \"Link\": r[\"Link\"],\n",
    "                \"Summary\": r[\"Summary\"]\n",
    "            })\n",
    "\n",
    "# Convert to plain dict for JSON serialization\n",
    "agg_out = {p: {t: vals for t, vals in types.items()} for p, types in agg.items()}\n",
    "\n",
    "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as fh:\n",
    "    json.dump(agg_out, fh, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Wrote aggregated summaries to\", OUTPUT_JSON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b884ccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 8 - quick check and example display\n",
    "import json\n",
    "with open(OUTPUT_JSON, encoding=\"utf-8\") as fh:\n",
    "    data = json.load(fh)\n",
    "\n",
    "# Show provinces that have entries\n",
    "for prov, types in list(data.items())[:10]:\n",
    "    print(prov, \"->\", {t: len(v) for t, v in types.items()})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
